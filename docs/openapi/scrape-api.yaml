create-scrape-job:
  post:
    tags:
      - scrape
    security:
      - api_key: [ ]
      - ims_key: [ ]
    summary: Create a new async URL Scrape job
    description: |
      ⚠️ **EXPERIMENTAL / PROTOTYPE**
      This endpoint allows you to submit a new async url scrape job.
      The job is provided a list URLs to be scraped. The URLs are scraped in parallel.
      The maximum number of URLs that can be scraped in a single job is 10.
      The returned jobId can be used to poll for the status and the resultsof the job.
    operationId: createScrapeJob
    requestBody:
      content:
        application/json:
          schema:
            $ref: './schemas.yaml#/ScrapeRequest'
    responses:
      '202':
        description: Job accepted
        content:
          application/json:
            schema:
              $ref: './schemas.yaml#/CreateAsyncJobAcceptedResponse'
      '400':
        $ref: './responses.yaml#/400-max-scrape-urls-exceeded'
      '401':
        $ref: './responses.yaml#/401'
      '500':
        $ref: './responses.yaml#/500'

get-scrape-job-status:
  parameters:
    - $ref: './parameters.yaml#/jobId'
  get:
    tags:
      - scrape
    security:
      - api_key: [ ]
      - ims_key: [ ]
    summary: Get scrape job status and result
    description: |
      ⚠️ **EXPERIMENTAL / PROTOTYPE**
      This endpoint is used to retrieve the status and results of an async scrape job.
      The jobId must be provided in the path parameter.
      If the job is completed, the result will be included in the response.
    operationId: getScrapeJobStatus
    responses:
      '200':
        description: Job status and (if available) result
        content:
          application/json:
            schema:
              $ref: './schemas.yaml#/ScrapeResponse'
      '404':
        description: Job not found

